import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import dns.resolver
from collections import defaultdict
import sys

class WebsiteAnalyzer:
    def __init__(self, url, max_depth=3, max_pages=500):
        self.url = url.rstrip('/')
        self.domain = self.extract_domain(url)
        self.base_domain = self.extract_base_domain(url)
        self.found_subdomains = set()
        self.found_subdirectories = set()
        self.found_pages = set()
        self.visited_urls = set()
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def extract_domain(self, url):
        """URL ‡§∏‡•á full domain extract ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        parsed = urlparse(url)
        return parsed.netloc

    def extract_base_domain(self, url):
        """URL ‡§∏‡•á base domain extract ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (without subdomain)"""
        parsed = urlparse(url)
        domain_parts = parsed.netloc.split('.')
        if len(domain_parts) >= 2:
            return '.'.join(domain_parts[-2:])
        return parsed.netloc

    def is_valid_url(self, url):
        """Check ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø URL valid ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and bool(parsed.scheme)
        except:
            return False

    def get_page_content(self, url, timeout=10):
        """Page ‡§ï‡§æ content get ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        try:
            response = self.session.get(url, timeout=timeout, allow_redirects=True)
            if response.status_code == 200:
                return response.text
        except Exception as e:
            print(f"   ‚ùå Error accessing {url}: {str(e)[:50]}...")
        return None

    def extract_links_from_page(self, url, html_content):
        """Page ‡§∏‡•á ‡§∏‡§≠‡•Ä links extract ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        links = set()
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ‡§∏‡§≠‡•Ä anchor tags find ‡§ï‡§∞‡•á‡§Ç
            for tag in soup.find_all(['a', 'link'], href=True):
                href = tag.get('href', '').strip()
                if href:
                    full_url = urljoin(url, href)
                    if self.is_same_domain(full_url):
                        # Remove fragments and query parameters for classification
                        parsed = urlparse(full_url)
                        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                        if clean_url != url and clean_url not in self.visited_urls:
                            links.add(clean_url)
            
            # Meta redirects ‡§≠‡•Ä check ‡§ï‡§∞‡•á‡§Ç
            meta_refresh = soup.find('meta', attrs={'http-equiv': 'refresh'})
            if meta_refresh and 'content' in meta_refresh.attrs:
                content = meta_refresh['content']
                url_match = re.search(r'url=(.+)', content, re.IGNORECASE)
                if url_match:
                    redirect_url = urljoin(url, url_match.group(1))
                    if self.is_same_domain(redirect_url):
                        links.add(redirect_url)
                        
        except Exception as e:
            print(f"   ‚ùå Error parsing {url}: {str(e)[:50]}...")
        
        return links

    def is_same_domain(self, url):
        """Check ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø URL same domain ‡§ï‡§æ ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç"""
        try:
            parsed = urlparse(url)
            url_domain = parsed.netloc
            return self.base_domain in url_domain or url_domain in self.domain
        except:
            return False

    def classify_url(self, url):
        """URL ‡§ï‡•ã classify ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (subdomain, subdirectory, ‡§Ø‡§æ page)"""
        parsed = urlparse(url)
        domain = parsed.netloc
        path = parsed.path.rstrip('/')
        
        # Check if it's a subdomain
        if domain != self.domain and self.base_domain in domain:
            self.found_subdomains.add(domain)
            return "subdomain"
        
        # Check if it's a subdirectory or page
        if path:
            path_parts = [part for part in path.split('/') if part]
            if path_parts:
                # If path ends with common file extensions, it's a page
                if any(path.lower().endswith(ext) for ext in ['.html', '.htm', '.php', '.asp', '.jsp', '.py']):
                    self.found_pages.add(url)
                    # Also add its directory
                    if len(path_parts) > 1:
                        dir_path = '/'.join(path_parts[:-1])
                        self.found_subdirectories.add(dir_path)
                    return "page"
                else:
                    # It's a directory
                    self.found_subdirectories.add(path.lstrip('/'))
                    return "subdirectory"
        
        return "page"

    def crawl_website(self, start_url=None, depth=0):
        """Website ‡§ï‡•ã crawl ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        if start_url is None:
            start_url = self.url
            
        if depth > self.max_depth or len(self.visited_urls) >= self.max_pages:
            return
        
        if start_url in self.visited_urls:
            return
            
        self.visited_urls.add(start_url)
        print(f"   üîç Crawling (depth {depth}): {start_url}")
        
        # Page content get ‡§ï‡§∞‡•á‡§Ç
        html_content = self.get_page_content(start_url)
        if not html_content:
            return
        
        # URL ‡§ï‡•ã classify ‡§ï‡§∞‡•á‡§Ç
        self.classify_url(start_url)
        
        # ‡§∏‡§≠‡•Ä links extract ‡§ï‡§∞‡•á‡§Ç
        links = self.extract_links_from_page(start_url, html_content)
        
        # Recursively crawl found links
        for link in links:
            if len(self.visited_urls) < self.max_pages:
                self.crawl_website(link, depth + 1)
            else:
                break

    def find_subdomains_dns(self):
        """DNS ‡§ï‡•á through subdomains find ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        print("üîç DNS ‡§∏‡•á subdomains ‡§¢‡•Ç‡§Ç‡§ß ‡§∞‡§π‡•á ‡§π‡•à‡§Ç...")
        
        common_subdomains = [
            'www', 'mail', 'ftp', 'blog', 'shop', 'store', 'news',
            'support', 'help', 'forum', 'api', 'docs', 'cdn', 'static',
            'images', 'img', 'media', 'admin', 'login', 'secure',
            'app', 'mobile', 'm', 'test', 'dev', 'staging', 'demo',
            'portal', 'client', 'customer', 'member', 'user', 'status',
            'beta', 'alpha', 'v2', 'download', 'files', 'bugs', 'issues'
        ]
        
        def check_subdomain(subdomain):
            try:
                full_subdomain = f"{subdomain}.{self.base_domain}"
                dns.resolver.resolve(full_subdomain, 'A')
                
                # HTTP request ‡§≠‡•Ä ‡§≠‡•á‡§ú‡•á‡§Ç
                for protocol in ['https', 'http']:
                    try:
                        test_url = f"{protocol}://{full_subdomain}"
                        response = self.session.head(test_url, timeout=5, allow_redirects=True)
                        if response.status_code < 400:
                            return full_subdomain
                    except:
                        continue
            except:
                pass
            return None
        
        # Concurrent checking
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_subdomain = {executor.submit(check_subdomain, sub): sub for sub in common_subdomains}
            
            for future in as_completed(future_to_subdomain):
                result = future.result()
                if result:
                    self.found_subdomains.add(result)
                    print(f"   ‚úÖ Found subdomain: {result}")

    def analyze_deep(self):
        """Deep analysis ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        print(f"üåê Deep Website Analysis ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç: {self.url}")
        print(f"üìç Domain: {self.domain}")
        print(f"üèóÔ∏è Base Domain: {self.base_domain}")
        print(f"üî¢ Max Depth: {self.max_depth}, Max Pages: {self.max_pages}")
        print("="*80)
        
        # ‡§™‡§π‡§≤‡•á DNS ‡§∏‡•á subdomains find ‡§ï‡§∞‡•á‡§Ç
        self.find_subdomains_dns()
        print()
        
        # ‡§´‡§ø‡§∞ website ‡§ï‡•ã crawl ‡§ï‡§∞‡•á‡§Ç
        print("üï∑Ô∏è Website crawling ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç...")
        self.crawl_website()
        print()
        
        self.print_detailed_results()

    def print_detailed_results(self):
        """Detailed results print ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        print("="*80)
        print("üìä DETAILED ANALYSIS RESULTS")
        print("="*80)
        
        print(f"\nüè† Main Website: {self.url}")
        print(f"üåê Full Domain: {self.domain}")
        print(f"üèóÔ∏è Base Domain: {self.base_domain}")
        
        # Subdomains results
        print(f"\nüîó SUBDOMAINS FOUND ({len(self.found_subdomains)}):")
        if self.found_subdomains:
            for subdomain in sorted(self.found_subdomains):
                print(f"   ‚îú‚îÄ‚îÄ https://{subdomain}")
                print(f"   ‚îÇ   ‚îî‚îÄ‚îÄ Type: Subdomain")
        else:
            print("   ‚ùå ‡§ï‡•ã‡§à subdomains ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á")
        
        # Subdirectories results with categorization
        subdirs_by_level = defaultdict(list)
        for subdir in self.found_subdirectories:
            level = len([p for p in subdir.split('/') if p])
            subdirs_by_level[level].append(subdir)
        
        print(f"\nüìÅ SUBDIRECTORIES/FOLDERS FOUND ({len(self.found_subdirectories)}):")
        if self.found_subdirectories:
            for level in sorted(subdirs_by_level.keys()):
                print(f"\n   üìÇ Level {level} Directories:")
                for subdir in sorted(subdirs_by_level[level]):
                    full_path = f"{self.url}/{subdir.lstrip('/')}"
                    print(f"      ‚îú‚îÄ‚îÄ {full_path}")
                    print(f"      ‚îÇ   ‚îî‚îÄ‚îÄ Type: Directory/Folder (Level {level})")
        else:
            print("   ‚ùå ‡§ï‡•ã‡§à subdirectories ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á")
        
        # Pages found
        print(f"\nüìÑ PAGES FOUND ({len(self.found_pages)}):")
        if self.found_pages:
            pages_by_type = defaultdict(list)
            for page in self.found_pages:
                if page.endswith('.html') or page.endswith('.htm'):
                    pages_by_type['HTML'].append(page)
                elif page.endswith('.php'):
                    pages_by_type['PHP'].append(page)
                else:
                    pages_by_type['Other'].append(page)
            
            for page_type, pages in pages_by_type.items():
                if pages:
                    print(f"\n   üìù {page_type} Pages ({len(pages)}):")
                    for page in sorted(pages)[:20]:  # Show only first 20
                        print(f"      ‚îú‚îÄ‚îÄ {page}")
                        print(f"      ‚îÇ   ‚îî‚îÄ‚îÄ Type: {page_type} Page")
                    if len(pages) > 20:
                        print(f"      ‚îî‚îÄ‚îÄ ... ‡§î‡§∞ {len(pages) - 20} pages")
        else:
            print("   ‚ùå ‡§ï‡•ã‡§à specific pages ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á")
        
        # All found URLs summary
        all_urls = set()
        all_urls.update(f"https://{sub}" for sub in self.found_subdomains)
        all_urls.update(f"{self.url}/{subdir.lstrip('/')}" for subdir in self.found_subdirectories)
        all_urls.update(self.found_pages)
        
        print(f"\nüåê ALL DISCOVERED URLs ({len(all_urls)}):")
        if len(all_urls) <= 50:
            for url in sorted(all_urls):
                print(f"   ‚îú‚îÄ‚îÄ {url}")
        else:
            for url in sorted(list(all_urls)[:25]):
                print(f"   ‚îú‚îÄ‚îÄ {url}")
            print(f"   ‚îú‚îÄ‚îÄ ... (showing first 25 of {len(all_urls)})")
            for url in sorted(list(all_urls)[-5:]):
                print(f"   ‚îú‚îÄ‚îÄ {url}")
        
        print(f"\nüìà SUMMARY:")
        print(f"   ‚Ä¢ Total Subdomains: {len(self.found_subdomains)}")
        print(f"   ‚Ä¢ Total Subdirectories: {len(self.found_subdirectories)}")
        print(f"   ‚Ä¢ Total Pages Found: {len(self.found_pages)}")
        print(f"   ‚Ä¢ Total URLs Crawled: {len(self.visited_urls)}")
        print(f"   ‚Ä¢ Total Unique Locations: {len(all_urls)}")

def main():
    print("üï∑Ô∏è  ADVANCED WEBSITE ANALYZER - SUBDOMAINS & DIRECTORIES")
    print("="*80)
    
    # User ‡§∏‡•á URL input ‡§≤‡•á‡§Ç
    url = input("üåê Website URL enter ‡§ï‡§∞‡•á‡§Ç (with http/https): ").strip()
    
    # URL validation
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    # Advanced options
    print("\n‚öôÔ∏è Advanced Options:")
    try:
        max_depth = int(input("üî¢ Maximum crawl depth (default 3): ") or "3")
        max_pages = int(input("üìÑ Maximum pages to crawl (default 500): ") or "500")
    except ValueError:
        max_depth = 3
        max_pages = 500
    
    try:
        # Analyzer create ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ run ‡§ï‡§∞‡•á‡§Ç
        analyzer = WebsiteAnalyzer(url, max_depth=max_depth, max_pages=max_pages)
        analyzer.analyze_deep()
        
        # Save results option
        save_option = input("\nüíæ Results ‡§ï‡•ã file ‡§Æ‡•á‡§Ç save ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç? (y/n): ").lower()
        if save_option == 'y':
            filename = f"website_analysis_{analyzer.base_domain.replace('.', '_')}.txt"
            # Here you can add file saving logic
            print(f"üìÅ Results saved to {filename}")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Analysis interrupted by user")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("‡§ï‡•É‡§™‡§Ø‡§æ valid URL enter ‡§ï‡§∞‡•á‡§Ç‡•§")

if __name__ == "__main__":
    main()
